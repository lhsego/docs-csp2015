## Housing Sales ##

### Data Description - Housing ###

The housing dataset contains data about housing sales aggregated to the 
county level in the United States between 2008-10-01 and 2014-03-01. These are 
[Zillow](http://zillow.com) data provided by [Quandl](https://www.quandl.com/c/housing). 
The data variables are as follows:

Variable | Description
---------|-----------------
fips | Federal Information Processing Standard, a 5 digit count code
county | US county name
state | US state name
time | date (the data is aggregated monthly)
nSold | number of homes sold this month
medListPriceSqft | median list price per square foot
medSoldPriceSqft | median sold price per square foot

### Trelliscope View - Housing ###

The purpose of this activity is to launch the `trelliscope` display and then explore the housing sales data: 
to look for patterns, trends, anomalies, etc. 
In this section, we'll show you the code required to launch a pre-created `trelliscope` view of the
data.

Let's begin by setting the working directory for this example. You will have to change the path 
in the command below to correctly point to the `housing_demo` directory.

```{r eval = TRUE}
# Set the working directory. Edit "~/correct_path" as necessary
setwd("~/correct_path/housing_demo")
```

And load the `trelliscope` and `housingData` packages:

```{r eval = TRUE, results = "hide", message = FALSE}
# Load packages
library(trelliscope)
library(housingData)
```

The following will launch two pre-made trelliscope displays:
- median list/sold price per sq ft vs time
- median list/sold price per sq ft vs time by state

Each panel represents the median listed (blue) and sold (pink) housing value plotted over time. 
Note that some of the panels only have one of the two values.  

Use `Cntrl-C` or `ESC` to stop the viewer and return to the R prompt.

```{r eval = FALSE, results = "hide", message = FALSE}
# Open the connection to the pre-existing trelliscope visualization. "vdb_housing" is a folder
# in the "housing_demo" folder, where we set the working directory earlier.
vdbConn("vdb_housing")

# use this port when running locally on your own computer
myport <- 8100 

# Launch the trelliscope viewer.  Use Ctrl-C or ESC to stop the reviewer and return
# the R prompt
view(port = myport)
```

### Challenge Questions - Housing ###

In order to familiarize 
yourself with the interface and get a sense of the features of the `trelliscope` package, see if 
you can answer the following set of questions:

1. Can you find the county with the largest positive slope in list price per square foot?  The slope is from a straight line
regression model that measures linear trend over time. (Hint: use the Table Sort/Filter or Univariate Filter)

2. Can you find the county with the largest negative slope? 

3. Can you find the state that has the largest range of county trends in list price? 
(Hint: use the Bivariate Filter)

4. Can you find the county with the largest increasing price trend in the South? 
(Hint: use the Table Sort/Filter to filter first) 

### Code to Create Trelliscope View - Housing ###

This activity will teach you how to create your own `trelliscope` displays as well as some basic functionality of 
the `datadr` package using the housing data set. We will be using the same data from the pre-made displays, 
but now we will illustrate the code that generates the `trelliscope` view. 
This tutorial will demonstrate how to generate your own plots and how to create 
and use cognostics to filter and sort the plots.

There are several key steps to creating a Trelliscope view that we will discuss below:
- Load the `trelliscope` package and other requisite packages
- Divide the data using `divide()` from the `datadr` package
- Get an initial feel for the data using tabular summaries
- Define the **panel** and **cognostics** functions
- Establish a connection to a visualization data base (vdb) using `vdbConn()` from the 
  `trelliscope` package
- Write `trelliscope` view files to the vdb using `makeDisplay()`
- Launch the `trelliscope` viewer using `view()`

#### Preliminaries ####

Let's begin by removing any (and all) left-over objects in the Global environment of the R session:

```{r eval = FALSE, results = "hide"}
# Clear the R workspace
rm(list = ls())
```

Now we'll load the necessary libraries.  Note that the housing data loads automatically with
the `housingData` package.

```{r eval = TRUE, results = "hide", message = FALSE}
# Load the trelliscope package if you haven't already
library(trelliscope)
library(housingData)

# You will need to replace the first part of path with the location where you
# unzipped the demonstration files
setwd("~/correct_path/housing_demo")
```

Let's look at the first part of the housing data, along with it's structure

```{r eval = TRUE, message = FALSE}
# Display first 6 rows of housing data
head(housing)

# Display the dimensions, column names, and data types:
str(housing)
```

#### Divide the Data ####

Using the `divide()` function of the `datadr` package, we are going to divide the data set 
by the variables **county** and **state** and thereby create a divided data frame (DDF), 
one of the primary data types in the `datadr` package.  Each element in the DDF is as subset of the
original housing dataset, containing the data for a unique county and state combination. 

```{r eval = TRUE, message = FALSE}
# We will create a distributed data frame by dividing our data by county and state
byCounty <- divide(housing, by = c("county", "state"), update = TRUE)

# Note that "byCounty" is a distributed data frame (DDF)
class(byCounty)
```

**DDF** objects have a printing method that shows the size of the object in terms of memory 
and other metadata related to the DDF:

```{r eval = TRUE}
# Show (print) the DDF object:
byCounty
```
Let's look at first division in the DDF. It will consist of a key-value pair.
The key should look something like "county=X|state=Y" and the value
should be the data frame corresponding to that key.

```{r eval = TRUE}
# Display the first division of the DDF
byCounty[[1]]
```

Exploratory data analysis often begins with computing a
variety of summary statistics.  The `datadr` package has some predefined functions 
for performing these calculations on DDFs:

```{r eval = FALSE}
# Display the number of data divisions, corresponding to the number of unique counties
length(byCounty) 

# Column names in the data
names(byCounty)

# Data division keys (state & county names in this example)
head(getKeys(byCounty))

# Look at summary statistics for each variable
summary(byCounty)
```

We can get a sense of the number of rows in each element of the DDF as well as the amount of 
memory taken up by each element using `splitRowDistn()` and `splitSizeDistn()`.

```{r eval = TRUE, message = FALSE, fig.width = 4.5, fig.height = 4.5}}
# Percentiles of number of rows per division
plot(splitRowDistn(byCounty), xlab = "Percentile", 
     ylab = "Number of rows per division", main = "Number of Rows")

# Percentiles of number of bytes per division
plot(splitSizeDistn(byCounty), xlab = "Percentile", 
     ylab = "Number of bytes per division", main = "Number of Bytes")
```

You can access the elements of the distributed data frame (DDF) using the key or by index, 
just like you would with a traditional list.

```{r eval = TRUE}
# A data division can be accessed by by its named key 
byCounty[["county=Benton County|state=WA"]]

# A data division can be accessed by index number
byCounty[[176]] 
```

Finally, you can use the `drQuantile()` function to compute the sample quantiles for the elements in the DDF object.

```{r eval = TRUE, fig.width = 4.5, fig.height = 4.5}
# Create a plot of the quantiles of median list price/sqft 
priceQ <- drQuantile(byCounty, var = "medListPriceSqft")
xyplot(q ~ fval, data = priceQ, main = "Median List Price/Sqft Quantiles")
```

#### Recombine the data ####

Suppose we are interested in the trend component of each time series in our DDF.
We can do this by creating a linear model for each subdivision and extracting the slope parameter. 
We wish to incorporate this information into our pre-existing DDF and use it in our analysis. 

We begin by creating a transformation function that will calculate the slope for each division and then
testing the function on a single division.

```{r eval = TRUE, message = FALSE, results = "hide"}
# Create a function to calculate a linear model for a division and extract the slope parameter
lmCoef <- function(x) {
   data.frame(getSplitVars(x), slope = coef(lm(medListPriceSqft ~ time, data = x))[2])
}

# Test lmCoef on one division
kvApply(byCounty[[176]], lmCoef)
```

The next step is to add the linear slope transformation function to the DDF and then recombine it
using row binding to form a regular data frame.

```{r eval = TRUE, message = TRUE}
# Add the function transform to the DDF
byCountySlope <- addTransform(byCounty, lmCoef)

# Now look at data with the transformation
byCountySlope[[176]]

# Recombine the slope data into a single data frame using the combRbind() method
countySlopes <- recombine(byCountySlope, combRbind)

# Look at the recombined data
head(countySlopes)
str(countySlopes)
```
Sometimes you will want to actively combine two distributed data objects/frames together to create 
a new data set. We will demonstrate how to perform these types of operations using the `drJoin()` function.
Let's begin by looking at the **geoCounty** data frame which is part of the `housingData` package.

```{r eval = TRUE, message = FALSE}

# Look at geoCounty which contains more information about US counties
head(geoCounty)

# Divide geoCounty on county and state just like we did with the housing data
geo <- divide(geoCounty, by = c("county", "state"))
geo[[1]]
```

Let's also include some population data for each county from Wikipedia.  This is included in the
 **wikiCounty** dataset, also a part of the `housingData` package.

```{r eval = TRUE, message = FALSE}
# Look at the Wikepedia county data
head(wikiCounty)

# Divide it by county/state
wikiByCounty <- divide(wikiCounty, by = c("county", "state"))
```
Now we will join the divided housing, geoCounty, and wikiCounty data together to create a 
distributed data object (DDO).  

```{r eval = TRUE, message = FALSE}
# Join housing, geoCounty, and wikiCounty for each county/state key
joinedData <- drJoin(housing = byCounty, slope = byCountySlope, geo = geo, wiki = wikiByCounty)

# Note that this is no longer a DDF, but now a DDO with a different structure
class(joinedData)

# Look at the 176th key-value pair
joinedData[[176]]
```

Note that we now have more key-value pairs than we originally had with the Zillow housing data:
```{r eval = TRUE}
# Display the number of key-value pairs
length(joinedData)
length(byCounty)
```
Note that the **geo** and **wikiByCounty** DDF's
have additional counties not included in the housing DDFs (**byCounty** and **byCountySlope**).
```{r eval = TRUE, message = FALSE}
# geo contains more 225 more keys (counties) than byCounty
length(setdiff(unlist(getKeys(geo)), unlist(getKeys(byCounty))))

# Likewise, wikiByCounty contains 260 more keys (counties) than byCounty
length(setdiff(unlist(getKeys(wikiByCounty)), unlist(getKeys(byCounty))))
```
These extra counties do not contain housing data.  We can see an example here:
```{r eval = TRUE, message = FALSE}
# Identify keys in the joined data that don't have housing data
findNoHousing <- drLapply(joinedData, function(x) is.null(x$housing), combine = combRbind)

# Look at the structure of the output
str(findNoHousing)

# Find some cases where there is no housing data
head(findNoHousing[findNoHousing$val,])

# Display a case with no housing data
joinedData[["county=Adams County|state=IA"]]
```
To remove these extra counties with no housing data, we can use `drFilter()` and apply a
function to each subset that will remove the keys (counties) with missing housing data

``{r eval = TRUE}
# Filter dataset to remove divisions without housing sales data
joinedData <- drFilter(joinedData, function(x) !is.null(x$housing))

# And now we're back to the original set of keys we had with the housing data
setequal(unlist(getKeys(joinedData)), unlist(getKeys(byCounty)))
```
CONTINUE HERE
The data are now ready to be ingested into `trelliscope`. To use `trelliscope`, the user 
needs to define 
a visual data base and create basic functions for plotting. On top of this, a user can develop and 
use cognostics to improve data interpretability. A cognostic is usually, but not always, a summary 
statistic or some form of metadata to be included along with plots. These values can be useful in 
determining patterns or anomalies in visual displays. 

```{r eval = F, message = F}

# Define a visualization database directory where the plots and metadata
# will be saved. Unless a complete file path is specified, the vdb will be
# generated in the working directory. 
vdbConn("vdb_housing", autoYes=TRUE)
```

```{r eval = T}
# Define a plot function
timePanel <- function(x) {
   xyplot(medListPriceSqft + medSoldPriceSqft ~ time,
      data = x$housing, auto.key = TRUE, ylab = "Price / Sq. Ft.")
}

# Test the plot function on a single division
kvApply(timePanel, joinedData[[176]])

```

We have defined a simple plot function. It would be useful to define a set of cognostics that 
can give us a large set of angles with which to attack the data anlysis problem at hand. Some 
simple cognostics are included in the trelliscope package such as `cogMean()` and `cogRange()` 
which are self-explanatory. You are free to define any other measure of the data using the `cog()` function.  

```{r eval = T}
# Define a cognostics function: this is used to define information and 
# statistics that will be available in the Trelliscope UI for sorting and 
# filtering and also to display/link useful meta information.
priceCog <- function(a) { 
   x <- a$housing
   st <- getSplitVar(a, "state")
   ct <- getSplitVar(a, "county")
   zillowString <- paste(ct, st)
   zillowString <- gsub(" ", "-", zillowString)
   list(
      fips = cog(x$fips[1], desc = "fips code"),
      region = cog(state.region[state.abb == ifelse(st == "DC", "MD", st)]),
      division = cog(state.division[state.abb == ifelse(st == "DC", "MD", st)]),
      slope = cog(a$slope$slope, desc = "list price slope"),
      meanList = cogMean(x$medListPriceSqft),
      meanSold = cogMean(x$medSoldPriceSqft),
      listRange = cogRange(x$medListPriceSqft),
      soldRange = cogRange(x$medSoldPriceSqft),
      nObs = cog(length(which(!is.na(x$medListPriceSqft))), 
         desc = "number of non-NA list prices"),
      lat = cog(a$geo$lat, desc = "county latitude"),
      lon = cog(a$geo$lon, desc = "county longitude"),
      pop2013 = cog(log10(a$wiki$pop2013), desc = "log base 10 population in 2013"),
      wikiHref = cogHref(a$wiki$href, "wiki link"),
      zillowHref = cogHref(sprintf("http://www.zillow.com/homes/%s_rb/", zillowString), "zillow link")
   )
}

# Test on a single division
kvApply(priceCog, joinedData[[176]])

```

```{r eval = F}
# Create the display: this creates and saves display files and information in
# the vdb directory defined above
makeDisplay(joinedData,
   name = "list_sold_vs_time_datadr_tut",
   desc = "List and sold price over time",
   panelFn = timePanel, 
   cogFn = priceCog,
   width = 400, height = 400,
   lims = list(x = "same"))

# Open Trelliscope in a browser
myport <- 8100 # use this when running locally
# myport <- Sys.getenv("TR_PORT") # use this on demo cluster
view(port=myport)
```
